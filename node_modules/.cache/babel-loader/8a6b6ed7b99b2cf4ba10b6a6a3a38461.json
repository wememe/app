{"ast":null,"code":"'use strict';\n\nvar _slicedToArray = require(\"/Users/kenzo/Desktop/3box-dapp/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/slicedToArray\");\n\nvar promisify = require('promisify-es6');\n\nvar waterfall = require('async/waterfall');\n\nvar parallel = require('async/parallel');\n\nvar _require = require('./utils'),\n    createLock = _require.createLock,\n    updateMfsRoot = _require.updateMfsRoot,\n    validatePath = _require.validatePath,\n    traverseTo = _require.traverseTo,\n    addLink = _require.addLink,\n    updateTree = _require.updateTree,\n    toPullSource = _require.toPullSource,\n    loadNode = _require.loadNode,\n    limitStreamBytes = _require.limitStreamBytes,\n    countStreamBytes = _require.countStreamBytes,\n    zeros = _require.zeros;\n\nvar _require2 = require('ipfs-unixfs'),\n    unmarshal = _require2.unmarshal;\n\nvar pull = require('pull-stream/pull');\n\nvar cat = require('pull-cat');\n\nvar collect = require('pull-stream/sinks/collect');\n\nvar empty = require('pull-stream/sources/empty');\n\nvar err = require('pull-stream/sources/error');\n\nvar log = require('debug')('ipfs:mfs:write');\n\nvar values = require('pull-stream/sources/values');\n\nvar _require3 = require('ipfs-unixfs-engine'),\n    exporter = _require3.exporter,\n    importer = _require3.importer;\n\nvar deferred = require('pull-defer');\n\nvar CID = require('cids');\n\nvar defaultOptions = {\n  offset: 0,\n  // the offset in the file to begin writing\n  length: undefined,\n  // how many bytes from the incoming buffer to write\n  create: false,\n  // whether to create the file if it does not exist\n  truncate: false,\n  // whether to truncate the file first\n  rawLeaves: false,\n  reduceSingleLeafToSelf: false,\n  cidVersion: 0,\n  hashAlg: 'sha2-256',\n  format: 'dag-pb',\n  parents: false,\n  // whether to create intermediate directories if they do not exist\n  progress: undefined,\n  strategy: 'trickle',\n  flush: true,\n  leafType: 'raw'\n};\n\nmodule.exports = function mfsWrite(ipfs) {\n  return promisify(function (path, content, options, callback) {\n    if (typeof options === 'function') {\n      callback = options;\n      options = {};\n    }\n\n    options = Object.assign({}, defaultOptions, options);\n\n    if (options.offset < 0) {\n      return callback(new Error('cannot have negative write offset'));\n    }\n\n    if (options.length < 0) {\n      return callback(new Error('cannot have negative byte count'));\n    }\n\n    if (!options.length && options.length !== 0) {\n      options.length = Infinity;\n    }\n\n    options.cidVersion = options.cidVersion || 0;\n    waterfall([function (done) {\n      parallel([function (next) {\n        return toPullSource(content, options, next);\n      }, function (next) {\n        return validatePath(path, next);\n      }], done);\n    }, // walk the mfs tree to the containing folder node\n    function (_ref, done) {\n      var _ref2 = _slicedToArray(_ref, 2),\n          source = _ref2[0],\n          path = _ref2[1];\n\n      waterfall([function (next) {\n        var opts = Object.assign({}, options, {\n          createLastComponent: options.parents\n        });\n\n        if (opts.createLastComponent) {\n          createLock().writeLock(function (callback) {\n            traverseTo(ipfs, path.directory, opts, function (error, result) {\n              return callback(error, {\n                source: source,\n                containingFolder: result\n              });\n            });\n          })(next);\n        } else {\n          createLock().readLock(function (callback) {\n            traverseTo(ipfs, path.directory, opts, function (error, result) {\n              return callback(error, {\n                source: source,\n                containingFolder: result\n              });\n            });\n          })(next);\n        }\n      }, function (_ref3, next) {\n        var source = _ref3.source,\n            containingFolder = _ref3.containingFolder;\n        updateOrImport(ipfs, options, path, source, containingFolder, next);\n      }], done);\n    }], function (error) {\n      return callback(error);\n    });\n  });\n};\n\nvar updateOrImport = function updateOrImport(ipfs, options, path, source, containingFolder, callback) {\n  waterfall([function (next) {\n    var existingChild = containingFolder.node.links.reduce(function (last, child) {\n      if (child.name === path.name) {\n        return child;\n      }\n\n      return last;\n    }, null);\n\n    if (existingChild) {\n      return loadNode(ipfs, existingChild, next);\n    }\n\n    if (!options.create) {\n      return next(new Error('file does not exist'));\n    }\n\n    next(null, null);\n  }, function (existingChild, next) {\n    write(ipfs, existingChild, source, options, next);\n  }, // The slow bit is done, now add or replace the DAGLink in the containing directory\n  // re-reading the path to the containing folder in case it has changed in the interim\n  function (child, next) {\n    createLock().writeLock(function (callback) {\n      var opts = Object.assign({}, options, {\n        createLastComponent: options.parents\n      });\n      traverseTo(ipfs, path.directory, opts, function (error, containingFolder) {\n        if (error) {\n          return callback(error);\n        }\n\n        waterfall([function (next) {\n          addLink(ipfs, {\n            parent: containingFolder.node,\n            name: path.name,\n            child: {\n              multihash: child.multihash || child.hash,\n              size: child.size\n            },\n            flush: options.flush\n          }, function (error, newContaingFolder) {\n            // Store new containing folder CID\n            containingFolder.node = newContaingFolder;\n            next(error);\n          });\n        }, // Update the MFS tree from the containingFolder upwards\n        function (next) {\n          return updateTree(ipfs, containingFolder, next);\n        }, // Update the MFS record with the new CID for the root of the tree\n        function (newRoot, next) {\n          return updateMfsRoot(ipfs, newRoot.node.multihash, next);\n        }], function (error, result) {\n          callback(error, result);\n        });\n      });\n    })(next);\n  }], callback);\n};\n\nvar write = function write(ipfs, existingNode, source, options, callback) {\n  var existingNodeCid;\n  var existingNodeMeta;\n\n  if (existingNode) {\n    existingNodeCid = new CID(existingNode.multihash);\n    existingNodeMeta = unmarshal(existingNode.data);\n    log(\"Overwriting file \".concat(existingNodeCid.toBaseEncodedString(), \" offset \").concat(options.offset, \" length \").concat(options.length));\n  } else {\n    log(\"Writing file offset \".concat(options.offset, \" length \").concat(options.length));\n  }\n\n  var sources = []; // pad start of file if necessary\n\n  if (options.offset > 0) {\n    if (existingNode && existingNodeMeta.fileSize() > options.offset) {\n      log(\"Writing first \".concat(options.offset, \" bytes of original file\"));\n      var startFile = deferred.source();\n      sources.push(startFile);\n      pull(exporter(existingNodeCid, ipfs.dag, {\n        offset: 0,\n        length: options.offset\n      }), collect(function (error, files) {\n        if (error) {\n          return startFile.resolve(err(error));\n        }\n\n        startFile.resolve(files[0].content);\n      }));\n    } else {\n      log(\"Writing zeros for first \".concat(options.offset, \" bytes\"));\n      sources.push(zeros(options.offset));\n    }\n  }\n\n  var endFile = deferred.source(); // add the new source\n\n  sources.push(pull(source, limitStreamBytes(options.length), countStreamBytes(function (bytesRead) {\n    log(\"Wrote \".concat(bytesRead, \" bytes\"));\n\n    if (existingNode && !options.truncate) {\n      // if we've done reading from the new source and we are not going\n      // to truncate the file, add the end of the existing file to the output\n      var fileSize = existingNodeMeta.fileSize();\n      var offset = options.offset + bytesRead;\n\n      if (fileSize > offset) {\n        log(\"Writing last \".concat(fileSize - offset, \" of \").concat(fileSize, \" bytes from original file\"));\n        pull(exporter(existingNodeCid, ipfs.dag, {\n          offset: offset\n        }), collect(function (error, files) {\n          if (error) {\n            return endFile.resolve(err(error));\n          }\n\n          endFile.resolve(files[0].content);\n        }));\n      } else {\n        log(\"Not writing last bytes from original file\");\n        endFile.resolve(empty());\n      }\n    }\n  }))); // add the end of the file if necessary\n\n  if (existingNode && !options.truncate) {\n    sources.push(endFile);\n  }\n\n  pull(values([{\n    path: '',\n    content: cat(sources)\n  }]), importer(ipfs.dag, {\n    progress: options.progress,\n    hashAlg: options.hash,\n    cidVersion: options.cidVersion,\n    strategy: options.strategy,\n    rawLeaves: options.rawLeaves,\n    reduceSingleLeafToSelf: options.reduceSingleLeafToSelf,\n    leafType: options.leafType\n  }), collect(function (error, results) {\n    if (error) {\n      return callback(error);\n    }\n\n    var result = results.pop();\n    log(\"Wrote \".concat(new CID(result.multihash).toBaseEncodedString()));\n    callback(null, result);\n  }));\n};","map":null,"metadata":{},"sourceType":"script"}