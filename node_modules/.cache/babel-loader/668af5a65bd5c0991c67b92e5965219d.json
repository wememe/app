{"ast":null,"code":"'use strict';\n\nvar extend = require('deep-extend');\n\nvar UnixFS = require('ipfs-unixfs');\n\nvar pull = require('pull-stream');\n\nvar through = require('pull-through');\n\nvar parallel = require('async/parallel');\n\nvar waterfall = require('async/waterfall');\n\nvar dagPB = require('ipld-dag-pb');\n\nvar CID = require('cids');\n\nvar multihash = require('multihashing-async');\n\nvar reduce = require('./reduce');\n\nvar DAGNode = dagPB.DAGNode;\nvar defaultOptions = {\n  chunkerOptions: {\n    maxChunkSize: 262144,\n    avgChunkSize: 262144\n  },\n  rawLeaves: false,\n  hashAlg: 'sha2-256',\n  leafType: 'file',\n  cidVersion: 0\n};\n\nmodule.exports = function builder(createChunker, ipld, createReducer, _options) {\n  var options = extend({}, defaultOptions, _options);\n  options.cidVersion = options.cidVersion || options.cidVersion;\n  options.hashAlg = options.hashAlg || defaultOptions.hashAlg;\n\n  if (options.hashAlg !== 'sha2-256') {\n    options.cidVersion = 1;\n  }\n\n  return function (source) {\n    return function (items, cb) {\n      parallel(items.map(function (item) {\n        return function (cb) {\n          if (!item.content) {\n            // item is a directory\n            return createAndStoreDir(item, function (err, node) {\n              if (err) {\n                return cb(err);\n              }\n\n              if (node) {\n                source.push(node);\n              }\n\n              cb();\n            });\n          } // item is a file\n\n\n          createAndStoreFile(item, function (err, node) {\n            if (err) {\n              return cb(err);\n            }\n\n            if (node) {\n              source.push(node);\n            }\n\n            cb();\n          });\n        };\n      }), cb);\n    };\n  };\n\n  function createAndStoreDir(item, callback) {\n    // 1. create the empty dir dag node\n    // 2. write it to the dag store\n    var d = new UnixFS('directory');\n    waterfall([function (cb) {\n      return DAGNode.create(d.marshal(), [], options.hashAlg, cb);\n    }, function (node, cb) {\n      if (options.onlyHash) {\n        return cb(null, node);\n      }\n\n      var cid = new CID(options.cidVersion, 'dag-pb', node.multihash);\n      node = new DAGNode(node.data, node.links, node.serialized, cid);\n      ipld.put(node, {\n        cid: cid\n      }, function (err) {\n        return cb(err, node);\n      });\n    }], function (err, node) {\n      if (err) {\n        return callback(err);\n      }\n\n      callback(null, {\n        path: item.path,\n        multihash: node.multihash,\n        size: node.size\n      });\n    });\n  }\n\n  function createAndStoreFile(file, callback) {\n    if (Buffer.isBuffer(file.content)) {\n      file.content = pull.values([file.content]);\n    }\n\n    if (typeof file.content !== 'function') {\n      return callback(new Error('invalid content'));\n    }\n\n    var reducer = createReducer(reduce(file, ipld, options), options);\n    var chunker;\n\n    try {\n      chunker = createChunker(options.chunkerOptions);\n    } catch (error) {\n      return callback(error);\n    }\n\n    var previous;\n    var count = 0;\n    pull(file.content, chunker, pull.map(function (chunk) {\n      if (options.progress && typeof options.progress === 'function') {\n        options.progress(chunk.byteLength);\n      }\n\n      return Buffer.from(chunk);\n    }), pull.asyncMap(function (buffer, callback) {\n      if (options.rawLeaves) {\n        return multihash(buffer, options.hashAlg, function (error, hash) {\n          if (error) {\n            return callback(error);\n          }\n\n          return callback(null, {\n            multihash: hash,\n            size: buffer.length,\n            leafSize: buffer.length,\n            cid: new CID(1, 'raw', hash),\n            data: buffer\n          });\n        });\n      }\n\n      var file = new UnixFS(options.leafType, buffer);\n      DAGNode.create(file.marshal(), [], options.hashAlg, function (err, node) {\n        if (err) {\n          return callback(err);\n        }\n\n        callback(null, {\n          multihash: node.multihash,\n          size: node.size,\n          leafSize: file.fileSize(),\n          cid: new CID(options.cidVersion, 'dag-pb', node.multihash),\n          data: node\n        });\n      });\n    }), pull.asyncMap(function (leaf, callback) {\n      if (options.onlyHash) {\n        return callback(null, leaf);\n      }\n\n      ipld.put(leaf.data, {\n        cid: leaf.cid\n      }, function (error) {\n        return callback(error, leaf);\n      });\n    }), pull.map(function (leaf) {\n      return {\n        path: file.path,\n        multihash: leaf.cid.buffer,\n        size: leaf.size,\n        leafSize: leaf.leafSize,\n        name: '',\n        cid: leaf.cid\n      };\n    }), through( // mark as single node if only one single node\n    function onData(data) {\n      count++;\n\n      if (previous) {\n        this.queue(previous);\n      }\n\n      previous = data;\n    }, function ended() {\n      if (previous) {\n        if (count === 1) {\n          previous.single = true;\n        }\n\n        this.queue(previous);\n      }\n\n      this.queue(null);\n    }), reducer, pull.collect(function (err, roots) {\n      if (err) {\n        callback(err);\n      } else {\n        callback(null, roots[0]);\n      }\n    }));\n  }\n};","map":null,"metadata":{},"sourceType":"script"}